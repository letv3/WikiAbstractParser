{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time, os, glob,re\n",
    "import pyspark.sql.functions as sparkfunctions\n",
    "from pyspark.sql import SparkSession, DataFrame\n",
    "from pyspark.sql.types import *\n",
    "from pyspark import SparkContext, SparkConf\n",
    "from functools import reduce\n",
    "\n",
    "default_dir = \"d:/Study/STU FIIT/ZS 21-22/Information Retrival/wikidata\"\n",
    "\n",
    "link_to_full_wiki = default_dir + \"/whole_wiki/enwiki-20211101-pages-articles.xml.bz2\"\n",
    "\n",
    "#input paths\n",
    "input_abstracts_data_dir = default_dir + \"/wiki_articles\"\n",
    "input_default_abstracts_file = default_dir + \"/enwiki-latest-abstract.xml\"\n",
    "input_dbpedia_abstracts_file = default_dir + \"/long-abstracts_lang=en.ttl\"\n",
    "\n",
    "#output paths\n",
    "project_dir = \"../../data/document_base\"\n",
    "parsed_abstracts_dir = project_dir + \"/parsed_abstracts\"\n",
    "wikipedia_default_abstracts_dir = project_dir + \"/default_abstracts\"\n",
    "dbpedia_abstracts_dir = project_dir + \"/dbpedia\"\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# conf = SparkConf.setAll(pairs=[('spark.executor.memory', '8g'), ('spark.executor.cores', '3'), ('spark.cores.max', '3'), ('spark.driver.memory','8g')])\n",
    "spark = SparkSession \\\n",
    "    .builder \\\n",
    "    .appName('SparkParser').getOrCreate()\n",
    "    # .config(conf=conf)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def check_object(obj):\n",
    "    if isinstance(obj, str):\n",
    "        if obj != '':\n",
    "            return obj\n",
    "    return ''\n",
    "\n",
    "def parse_wiki_articles_file(spark, path):\n",
    "    abstract_pattern = r\"'''([^\\=]*)(?=(={1,6})([^\\n]+?)(={1,6})[ \\t]*(\\n|\\Z))\"\n",
    "    customSchema = StructType([\n",
    "        StructField('title', StringType()),\n",
    "        StructField('revision', StructType([\n",
    "            StructField('text', StructType([\n",
    "                StructField('_VALUE', StringType())\n",
    "            ]))\n",
    "        ]))\n",
    "    ])\n",
    "    df = spark.read.format('com.databricks.spark.xml')\\\n",
    "        .options(rowTag='page').load(path, schema=customSchema)\n",
    "    all_abstracts_rdd = df.rdd.map(\n",
    "        lambda loop: (\n",
    "            loop['title'],\n",
    "            abstract_text.group(0).replace('\\n', ' ')\n",
    "            if (abstract_text := re.search(pattern=abstract_pattern,\n",
    "                                           string=check_object(loop['revision']['text']['_VALUE']),\n",
    "                                           flags=re.MULTILINE | re.DOTALL)\n",
    "                ) is not None else None\n",
    "        ))\n",
    "    all_abstracts = all_abstracts_rdd.toDF(['title', 'abstract'])\n",
    "    valid_abstracts = all_abstracts.dropna()\n",
    "    valid_abstracts = valid_abstracts.filter(~valid_abstracts.title.contains(\"Wikipedia:Articles for deletion\"))\n",
    "    return valid_abstracts\n",
    "\n",
    "def parse_wiki_abstracts_file(spark, path):\n",
    "    title_pattern = r\"^Wikipedia: ([^\\n]*)\"\n",
    "    customSchema = StructType([\n",
    "        StructField('title', StringType(), False),\n",
    "        StructField('abstract', StringType(), False)\n",
    "    ])\n",
    "    df = spark.read.format(\"com.databricks.spark.xml\")\\\n",
    "        .options(rowTag='doc').load(path, schema=customSchema)\n",
    "    wiki_abstract_rdd = df.rdd.map(\n",
    "        lambda loop: (\n",
    "            re.search(pattern=title_pattern, string=loop['title']).group(1).replace('\\n', ' '),\n",
    "            loop['abstract']\n",
    "        ))\n",
    "    all_wiki_abstracts = wiki_abstract_rdd.toDF(['title', 'abstract'])\n",
    "    valid_abstracts = all_wiki_abstracts.dropna()\n",
    "    return valid_abstracts\n",
    "\n",
    "def parse_dbpedia_abstract_file(spark, path):\n",
    "    title_pattern = r\"^<http:\\/\\/dbpedia\\.org\\/resource\\/([^\\>]*)> <\"\n",
    "    abstract_pattern = re.compile(r'\"([^\"]*)\"')\n",
    "    sc = spark.sparkContext\n",
    "    rdd = sc.textFile(path)\n",
    "    rdd = rdd.map(\n",
    "        lambda loop: (\n",
    "            title.group(1).replace('_', ' ')\n",
    "            if (title := re.search(title_pattern, loop)) is not None else None,\n",
    "            abstract.group(1)\n",
    "            if (abstract := re.search(abstract_pattern, loop)) is not None else None\n",
    "        ))\n",
    "    df = rdd.toDF(['title', 'abstract'])\n",
    "    df = df.dropna()\n",
    "    return df\n",
    "\n",
    "def get_xml_filenames(file_dir):\n",
    "    os.chdir(file_dir)\n",
    "    all_files = glob.glob(f\"*.xml*\")\n",
    "    xml_files = []\n",
    "    for file_name in all_files:\n",
    "        if re.match(r\"(.*)(?=(\\.bz2))\", file_name): continue\n",
    "        else: xml_files.append(file_name)\n",
    "    return xml_files"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "## PARSE ABSTRACTS from articles\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Parse all wiki from single bz2 file\n",
    "\n",
    "single .bz2 file ~18Gb"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "all_wiki = parse_wiki_articles_file(spark,link_to_full_wiki)\n",
    "all_wiki_persisted = all_wiki.cache()\n",
    "all_wiki_persisted.count()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Multiple articles dumps"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "parisng time: 12.002042531967163\n",
      "parisng time: 2.601315975189209\n",
      "parisng time: 1.0388760566711426\n",
      "parisng time: 3.9985997676849365\n",
      "parisng time: 3.080911636352539\n",
      "parisng time: 2.360297203063965\n",
      "parisng time: 1.9365158081054688\n",
      "parisng time: 0.9534831047058105\n",
      "parisng time: 2.614497423171997\n",
      "elapsed: 30.588595151901245\n"
     ]
    }
   ],
   "source": [
    "# get all xml files from directory\n",
    "xml_files = get_xml_filenames(input_abstracts_data_dir)\n",
    "print(xml_files)\n",
    "start_time = time.time()\n",
    "all_dfs = []\n",
    "for idx, file in enumerate(xml_files):\n",
    "    absolute_path = input_abstracts_data_dir + \"/\" + file\n",
    "    mode = 'overwrite' if idx == 0 else 'append'\n",
    "    if os.path.exists(absolute_path):\n",
    "        start_time_single = time.time()\n",
    "        parsed_wikipedia_abstracts = parse_wiki_articles_file(spark, path=absolute_path)\n",
    "        all_dfs.append(parsed_wikipedia_abstracts)\n",
    "        print(f\"parisng time: {time.time() - start_time_single}\")\n",
    "\n",
    "print(f\"elapsed: {time.time() - start_time}\")\n",
    "\n",
    "# uniou all parsed df\n",
    "single_parsed_abstracts_df = reduce(DataFrame.union, all_dfs)\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### Parse default abstracts from wikipedia-abstracts-latest"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "elapsed: 2.980881690979004\n",
      "writing to csv elapsed: 2.980881690979004\n",
      "+--------------------+--------------------+\n",
      "|               title|            abstract|\n",
      "+--------------------+--------------------+\n",
      "|           Anarchism|Anarchism is a po...|\n",
      "|              Autism|| duration     =L...|\n",
      "|              Albedo|Albedo (; ) is th...|\n",
      "|                   A|           A-sharp}}|\n",
      "|             Alabama|(We dare defend o...|\n",
      "|            Achilles|In Greek mytholog...|\n",
      "|     Abraham Lincoln|| alt            ...|\n",
      "|           Aristotle|                  }}|\n",
      "|An American in Paris|An American in Pa...|\n",
      "|Academy Award for...|The Academy Award...|\n",
      "|      Academy Awards|             Oscar}}|\n",
      "|             Actrius|  | starring       =|\n",
      "|     Animalia (book)|Animalia is an il...|\n",
      "|International Ato...|International Ato...|\n",
      "|            Altruism|Altruism is the p...|\n",
      "|            Ayn Rand|| birth_place = S...|\n",
      "|        Alain Connes|| birth_place = D...|\n",
      "|          Allan Dwan|| birth_place  = ...|\n",
      "|             Algeria|| common_name = A...|\n",
      "|List of Atlas Shr...|This is a list of...|\n",
      "+--------------------+--------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "if os.path.exists(input_default_abstracts_file):\n",
    "    start_time = time.time()\n",
    "    wiki_abstracts = parse_wiki_abstracts_file(spark, path=input_default_abstracts_file)\n",
    "    print(f\"elapsed: {time.time() - start_time}\")\n",
    "    # wiki_abstracts.coalesce(1).write.csv(path=wikipedia_default_abstracts_dir, sep='\\t', header=False, mode='overwrite')\n",
    "    print(f\"writing to csv elapsed: {time.time() - start_time}\")\n",
    "    wiki_abstracts.show(20)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### Parse dbpedia abstracts from long_abstracts dbpedia"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "elapsed: 1.6530005931854248\n",
      "writing to csv elapsed: 1.6530005931854248\n",
      "+--------------------+--------------------+\n",
      "|               title|            abstract|\n",
      "+--------------------+--------------------+\n",
      "|     Animalia (book)|Animalia is an il...|\n",
      "|Agricultural science|Agricultural scie...|\n",
      "|              Albedo|Albedo () (Latin:...|\n",
      "|        Alain Connes|Alain Connes (Fre...|\n",
      "|International Ato...|International Ato...|\n",
      "|                   A|A or a is the fir...|\n",
      "|An American in Paris|An American in Pa...|\n",
      "|List of Atlas Shr...|This is a list of...|\n",
      "|          Allan Dwan|Allan Dwan (born ...|\n",
      "|          Astronomer|An astronomer is ...|\n",
      "|            Achilles|In Greek mytholog...|\n",
      "|           Anarchism|Anarchism is a po...|\n",
      "|        Anthropology|Anthropology is t...|\n",
      "|              Autism|Autism is a devel...|\n",
      "|      Academy Awards|The Academy Award...|\n",
      "|             Actrius|Actresses (Catala...|\n",
      "|        Answer (law)|In law, an Answer...|\n",
      "|Academy Award for...|The Academy Award...|\n",
      "|Appellate procedu...|United States app...|\n",
      "|     Abraham Lincoln|Abraham Lincoln (...|\n",
      "+--------------------+--------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "if os.path.exists(input_dbpedia_abstracts_file):\n",
    "    start_time = time.time()\n",
    "    dbpedia_abstracts = parse_dbpedia_abstract_file(spark, path=input_dbpedia_abstracts_file)\n",
    "    print(f\"elapsed: {time.time() - start_time}\")\n",
    "    # dbpedia_abstracts = dbpedia_abstracts_rdd.toDF(['title', 'abstract'])\n",
    "    # dbpedia_abstracts.coalesce(1).write.csv(path=dbpedia_abstracts_dir, sep='\\t', header=False, mode='overwrite')\n",
    "    print(f\"writing to csv elapsed: {time.time() - start_time}\")\n",
    "    dbpedia_abstracts.show(20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### Calculate similarity for abstracts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\ollyt\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "import string, re\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "nltk.download('stopwords')\n",
    "stopwords = stopwords.words('english')\n",
    "def clean_string(text):\n",
    "    text = ''.join([word for word in text if word not in string.punctuation])\n",
    "    text = text.lower()\n",
    "    text = ' '.join([word for word in text.split() if word not in stopwords])\n",
    "    return text\n",
    "\n",
    "def compare_texts(first_text, second_text) -> float:\n",
    "    \"\"\"function to compare similarity of two texts\"\"\"\n",
    "    \"\"\":return Float value from 0 to 1 which represent similarity of two strs\"\"\"\n",
    "    if first_text == None or first_text == '' \\\n",
    "        or second_text == None or second_text == '':\n",
    "        return 0.0\n",
    "    cleaned = list(map(clean_string, [first_text, second_text]))\n",
    "    for cleaned_str in cleaned:\n",
    "        if not cleaned_str or len(re.sub(r\"\\s+\", \"\", cleaned_str)) < 3:\n",
    "            return 0.0\n",
    "    vectorizer = CountVectorizer().fit_transform(cleaned)\n",
    "    vectors = vectorizer.toarray()\n",
    "    first_text = vectors[0].reshape(1, -1)\n",
    "    second_text = vectors[1].reshape(1, -1)\n",
    "    return float(cosine_similarity(first_text, second_text)[0][0])\n",
    "\n",
    "\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- dbpedia_title: string (nullable = true)\n",
      " |-- dbpedia_abstract: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "dbpedia_abstracts.printSchema()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- title: string (nullable = true)\n",
      " |-- abstract: string (nullable = true)\n",
      " |-- default_abstract: string (nullable = true)\n",
      " |-- dbpedia_abstract: string (nullable = true)\n",
      "\n",
      "3148413\n",
      "+--------------------+--------------------+--------------------+--------------------+\n",
      "|               title|            abstract|    default_abstract|    dbpedia_abstract|\n",
      "+--------------------+--------------------+--------------------+--------------------+\n",
      "|    \"Nowell Sing We\"|'''&#34;Nowell Si...|                null|                null|\n",
      "|\"There Are Things...|'''''\"There Are T...|\"There Are Things...|                null|\n",
      "|         & Yet & Yet|'''''& Yet & Yet'...|| rev2      = Pit...|& Yet & Yet is th...|\n",
      "|     '60s Vibrations|''''' '60s Vibrat...|'60s Vibrations w...|'60s Vibrations w...|\n",
      "|   (315898) 2008 QD4|'''{{mp|(315898) ...|                  }}|(315898) 2008 QD4...|\n",
      "|(He's a) Shape in...|'''Production''' ...|\"(He's a) Shape i...|                null|\n",
      "|    ...But Seriously|'''Musicians''' *...|| recorded   = Ap...|...But Seriously ...|\n",
      "|  .600 Nitro Express|'''.600 Nitro Exp...|The .600 Nitro Ex...|The .600 Nitro Ex...|\n",
      "|                 .nf|'''.nf''' is the ...|                   ||.nf is the Intern...|\n",
      "|                  07|'''07''' may refe...|    07 may refer to:|07 may refer to: ...|\n",
      "|10.5 cm Cannon Mo...|'''10.5&nbsp;cm C...|  |wars=World War II|The 10.5 cm Canno...|\n",
      "|         100 Mothers|'''100 Mothers'''...|100 Mothers was a...|100 Mothers was a...|\n",
      "|         100% (band)|''' | Featured ar...|               –2021|                null|\n",
      "|100% Scooter – 25...|'''''100% Scooter...|| recorded   = 19...|                null|\n",
      "|          1000 Fires|'''''1000 Fires''...|| recorded   = 19...|1000 Fires is the...|\n",
      "|104th Street stat...|'''104th Street''...|      104th Street}}|104th Street (sig...|\n",
      "|105th Siege Batte...|'''105th Siege Ba...|        |allegiance=|105th Siege Batte...|\n",
      "|                1090|'''1090''' ('''[[...|Year 1090 (MXC) w...|Year 1090 (MXC) w...|\n",
      "|10th Anniversary ...|'''''10th Anniver...|10th Anniversary ...|10th Anniversary ...|\n",
      "|     1115 in Ireland|'''1115 in Irelan...|Events from the y...|Events from the y...|\n",
      "+--------------------+--------------------+--------------------+--------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "wiki_abstracts = wiki_abstracts.withColumnRenamed('title', 'default_title').withColumnRenamed('abstract', 'default_abstract')\n",
    "dbpedia_abstracts = dbpedia_abstracts.withColumnRenamed('title', 'dbpedia_title').withColumnRenamed('abstract', 'dbpedia_abstract')\n",
    "res = all_wiki_persisted.join(wiki_abstracts, all_wiki_persisted.title == wiki_abstracts.default_title, 'left')\n",
    "res = res.join(dbpedia_abstracts, res.title == dbpedia_abstracts.dbpedia_title, 'left')\n",
    "res = res.drop('default_title', 'dbpedia_title')\n",
    "\n",
    "\n",
    "res.printSchema()\n",
    "res_persisted = res.cache()\n",
    "print(res_persisted.count())\n",
    "res_persisted.show()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "outputs": [],
   "source": [
    "df_to_operate = res_persisted"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+--------------------+--------------------+--------------------+------------------+------------------+\n",
      "|               title|            abstract|    default_abstract|    dbpedia_abstract|default_similarity|dbpedia_similarity|\n",
      "+--------------------+--------------------+--------------------+--------------------+------------------+------------------+\n",
      "|    \"Nowell Sing We\"|'''&#34;Nowell Si...|                null|                null|               0.0|               0.0|\n",
      "|\"There Are Things...|'''''\"There Are T...|\"There Are Things...|                null|               1.0|               0.0|\n",
      "|         & Yet & Yet|'''''& Yet & Yet'...|| rev2      = Pit...|& Yet & Yet is th...|               0.0|         0.8183171|\n",
      "|     '60s Vibrations|''''' '60s Vibrat...|'60s Vibrations w...|'60s Vibrations w...|         0.7342174|        0.82996094|\n",
      "|   (315898) 2008 QD4|'''{{mp|(315898) ...|                  }}|(315898) 2008 QD4...|               0.0|         0.7487049|\n",
      "|(He's a) Shape in...|'''Production''' ...|\"(He's a) Shape i...|                null|        0.13611777|               0.0|\n",
      "|    ...But Seriously|'''Musicians''' *...|| recorded   = Ap...|...But Seriously ...|               0.0|        0.04412507|\n",
      "|  .600 Nitro Express|'''.600 Nitro Exp...|The .600 Nitro Ex...|The .600 Nitro Ex...|        0.80178374|         0.9797959|\n",
      "|                 .nf|'''.nf''' is the ...|                   ||.nf is the Intern...|               0.0|               1.0|\n",
      "|                  07|'''07''' may refe...|    07 may refer to:|07 may refer to: ...|         0.5360563|        0.99568033|\n",
      "|10.5 cm Cannon Mo...|'''10.5&nbsp;cm C...|  |wars=World War II|The 10.5 cm Canno...|       0.117242076|         0.8991734|\n",
      "|         100 Mothers|'''100 Mothers'''...|100 Mothers was a...|100 Mothers was a...|               1.0|               0.6|\n",
      "|         100% (band)|''' | Featured ar...|               –2021|                null|               0.0|               0.0|\n",
      "|100% Scooter – 25...|'''''100% Scooter...|| recorded   = 19...|                null|        0.11547005|               0.0|\n",
      "|          1000 Fires|'''''1000 Fires''...|| recorded   = 19...|1000 Fires is the...|       0.022005942|        0.66837955|\n",
      "|104th Street stat...|'''104th Street''...|      104th Street}}|104th Street (sig...|        0.54997194|         0.9004748|\n",
      "|105th Siege Batte...|'''105th Siege Ba...|        |allegiance=|105th Siege Batte...|               0.0|        0.67576593|\n",
      "|                1090|'''1090''' ('''[[...|Year 1090 (MXC) w...|Year 1090 (MXC) w...|          0.876714|          0.876714|\n",
      "|10th Anniversary ...|'''''10th Anniver...|10th Anniversary ...|10th Anniversary ...|        0.83852804|        0.98130876|\n",
      "|     1115 in Ireland|'''1115 in Irelan...|Events from the y...|Events from the y...|        0.70710677|        0.70710677|\n",
      "+--------------------+--------------------+--------------------+--------------------+------------------+------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "custom_udf = sparkfunctions.udf(lambda col1, col2: compare_texts(col1, col2), FloatType())\n",
    "df_to_operate = df_to_operate\\\n",
    "    .withColumn('default_similarity', custom_udf(df_to_operate.abstract, df_to_operate.default_abstract))\\\n",
    "    .withColumn('dbpedia_similarity', custom_udf(df_to_operate.abstract, df_to_operate.dbpedia_abstract))\n",
    "\n",
    "df_to_operate.write.csv(path=default_dir+\"/combined_output_whole\", sep='\\t', mode='overwrite')\n",
    "df_to_operate.show()\n",
    "\n",
    "#job time 25 min"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "outputs": [
    {
     "data": {
      "text/plain": "3148413"
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_to_operate.count()\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}